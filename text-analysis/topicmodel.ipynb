{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topicmodel_mp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCcTzE17RdoNR0Z4cDKphD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armaank/dbn/blob/main/text-analysis/topicmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone repo\n",
        "%cd /content\n",
        "%rm -rf dbn\n",
        "!ls\n",
        "!git clone https://github.com/armaank/dbn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcYBdZ1gRniQ",
        "outputId": "e809f15b-8a93-46dc-f463-ca391661bbdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "gdrive\tsample_data\n",
            "Cloning into 'dbn'...\n",
            "remote: Enumerating objects: 12280, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 12280 (delta 49), reused 87 (delta 30), pack-reused 12157\u001b[K\n",
            "Receiving objects: 100% (12280/12280), 127.75 MiB | 12.86 MiB/s, done.\n",
            "Resolving deltas: 100% (554/554), done.\n",
            "Checking out files: 100% (377/377), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cd to text-analysis directory\n",
        "%cd ./dbn/text-analysis/\n",
        "%ls\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmBeoetXTr7Q",
        "outputId": "be295167-6397-463f-c7df-623497fd0aa5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dbn/text-analysis\n",
            "csvpreprocess.py  mplda.py          README.md\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/             nltk_download.py  requirements.txt\n",
            "datahandler.py    \u001b[01;34mnotebooks\u001b[0m/        stopwords.txt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: funcy==1.15 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.15)\n",
            "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.18.2)\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.8.3)\n",
            "Requirement already satisfied: ipython==7.16.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (7.16.1)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: jedi==0.18.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.18.0)\n",
            "Requirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (2.11.3)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.1)\n",
            "Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (3.3.4)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (3.5)\n",
            "Requirement already satisfied: numexpr==2.7.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.7.2)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (1.19.5)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.5)\n",
            "Requirement already satisfied: parso==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.8.1)\n",
            "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (0.7.5)\n",
            "Requirement already satisfied: Pillow==8.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (8.1.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.16 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (3.0.16)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (0.7.0)\n",
            "Requirement already satisfied: Pygments==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (2.8.0)\n",
            "Requirement already satisfied: pyLDAvis==3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (3.2.2)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (2.8.1)\n",
            "Requirement already satisfied: pytz==2021.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (2021.1)\n",
            "Requirement already satisfied: regex==2020.11.13 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (2020.11.13)\n",
            "Requirement already satisfied: scikit-learn==0.24.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 33)) (0.24.1)\n",
            "Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (1.5.4)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (0.0)\n",
            "Requirement already satisfied: smart-open==4.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (4.2.0)\n",
            "Requirement already satisfied: threadpoolctl==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (2.1.0)\n",
            "Requirement already satisfied: tqdm==4.58.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (4.58.0)\n",
            "Requirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (4.3.3)\n",
            "Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (0.2.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->-r requirements.txt (line 9)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2->-r requirements.txt (line 28)) (0.37.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install datasets \n",
        "!python3 ./data/arch-daily-text/get_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5zgleg6UDzo",
        "outputId": "dc2cb747-be60-4145-e314-fa464a127680"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports \n",
        "import json\n",
        "import multiprocessing \n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "from datahandler import DataHandler\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eKYiDfuT5tC",
        "outputId": "5111e812-a702-4f62-b7f2-13c6584a145f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fcns\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words()\n",
        "\n",
        "def filter_ngram(ngram: str, n: int) -> bool:\n",
        "    \"\"\"\n",
        "    This function filters stopwords anything that is not a noun \n",
        "    (or adjective-noun bigram) (stopwords are only filtered for bigrams)\n",
        "    PRON is pronoun\n",
        "    \"\"\"\n",
        "    tag = nltk.pos_tag(ngram) #nltk.pos_tag provides the part of speech of the ngram\n",
        "    if tag[0][1] not in [\"JJ\", \"NN\"] and tag[1][1] not in [\"NN\"]: #JJ is adjective, NN is noun\n",
        "        return False\n",
        "    if n == 2:\n",
        "        if ngram[0] in stopwords or ngram[1] in stopwords: #checks for stopwords\n",
        "            return False\n",
        "    if n == 3:\n",
        "        if (\n",
        "            ngram[0] in stopwords\n",
        "            or ngram[-1] in stopwords\n",
        "            or ngram[1] in stopwords\n",
        "        ):\n",
        "            return False\n",
        "    if \"n\" in ngram or \"t\" in ngram: #n is noun\n",
        "        return False\n",
        "    if \"PRON\" in ngram:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def merge_ngram(text: str, bigrams: str, trigrams: str) -> str:\n",
        "    \"\"\"\n",
        "    This function loops through text and merges bigrams and trigrams that have\n",
        "    already been identified\n",
        "    \"\"\"\n",
        "    for gram in trigrams:\n",
        "        text = text.replace(gram, \"_\".join(gram.split()))\n",
        "    for gram in bigrams:\n",
        "        text = text.replace(gram, \"_\".join(gram.split()))\n",
        "    return text\n",
        "\n",
        "\n",
        "def filter_stopwords(text: str) -> str:\n",
        "    \"\"\"\n",
        "    This function filters stopwords for all of the text\n",
        "    \"\"\"\n",
        "    return [\n",
        "        word for word in text.split() if word not in stopwords and len(word) > 2\n",
        "    ]\n",
        "\n",
        "\n",
        "def filter_pos(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Filters all non-nouns from text\n",
        "    \"\"\"\n",
        "    pos = nltk.pos_tag(text)\n",
        "    filtered = [word[0] for word in pos if word[1] in [\"NN\"]]\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "5Pasn-weWUUz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "data_dir = os.path.join(\"arch-daily-text\", \"preproc\")\n",
        "print(\"Loading corpus\")\n",
        "corpus = DataHandler(data_dir, seed)\n",
        "\n",
        "# print some various information from the corpus\n",
        "print(\"Total Word Count: {}\".format(corpus.total_words))\n",
        "print(\"Number of Docs in the Corpus: {}\".format(corpus.total_docs))\n",
        "\n",
        "docs_fpath = corpus.data.keys()\n",
        "\n",
        "# create dictionary for filename and text\n",
        "fpath_txt = {}\n",
        "for fpath in docs_fpath:\n",
        "    with open(fpath, \"r\") as f:\n",
        "        fpath_txt[fpath] = f.read()\n",
        "\n",
        "# make dataframe\n",
        "df = (pd.DataFrame.from_dict(fpath_txt, orient='index')\n",
        "         .reset_index().rename(index = str, columns = {'index': 'file_name', 0: 'text'}))\n",
        "\n",
        "corpus = df['text']\n",
        "print(\"Finished loading corpus\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtrtA4ceWkix",
        "outputId": "5afb95e4-bb89-4560-de9a-8ba3942b3d32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpus\n",
            "Total Word Count: 2220710\n",
            "Number of Docs in the Corpus: 5405\n",
            "Finished loading corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_bigram_frequency = 50\n",
        "\n",
        "# finds bigrams in document\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()  \n",
        "finder = nltk.collocations.BigramCollocationFinder.from_documents(\n",
        "    [doc.split() for doc in corpus]\n",
        ")\n",
        "# removes uncommon bigrams (appears fewer than min_bigram_frequency times)\n",
        "finder.apply_freq_filter(min_bigram_frequency)  \n",
        "# pmi = pointwise mutual information, measures how \"useful\" bigram is\n",
        "bigram_scores = finder.score_ngrams(bigram_measures.pmi) \n",
        "\n",
        "bigram_pmi = pd.DataFrame(bigram_scores)\n",
        "bigram_pmi.columns = [\"bigram\", \"pmi\"]\n",
        "bigram_pmi.sort_values(by=\"pmi\", axis=0, ascending=False, inplace=True)\n",
        "\n",
        "min_trigram_frequency = 50\n",
        "\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "finder = nltk.collocations.TrigramCollocationFinder.from_documents(\n",
        "    [doc.split() for doc in corpus]\n",
        ")\n",
        "finder.apply_freq_filter(min_trigram_frequency)\n",
        "trigram_scores = finder.score_ngrams(trigram_measures.pmi)\n",
        "\n",
        "trigram_pmi = pd.DataFrame(trigram_scores)\n",
        "trigram_pmi.columns = [\"trigram\", \"pmi\"]\n",
        "trigram_pmi.sort_values(by=\"pmi\", axis=0, ascending=False, inplace=True)\n",
        "\n",
        "print(\"cell done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8-anu71Wvpb",
        "outputId": "fe71eb46-7363-43a6-d68d-55f300e37307"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_pmi = 5\n",
        "max_ngrams = 500\n",
        "\n",
        "#Removes low pmi bigrams and trigrams\n",
        "filtered_bigram = bigram_pmi[\n",
        "    bigram_pmi.apply(\n",
        "        lambda bigram: filter_ngram(bigram[\"bigram\"], 2) and min_pmi > 5,\n",
        "        axis=1,\n",
        "    )\n",
        "][:max_ngrams]\n",
        "\n",
        "filtered_trigram = trigram_pmi[\n",
        "    trigram_pmi.apply(\n",
        "        lambda trigram: filter_ngram(trigram[\"trigram\"], 3) and min_pmi > 5,\n",
        "        axis=1,\n",
        "    )\n",
        "][:max_ngrams]\n",
        "\n",
        "bigrams = [\n",
        "    \" \".join(x)\n",
        "    for x in filtered_bigram.bigram.values\n",
        "    if len(x[0]) > 2 or len(x[1]) > 2\n",
        "]\n",
        "trigrams = [\n",
        "    \" \".join(x)\n",
        "    for x in filtered_trigram.trigram.values\n",
        "    if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2\n",
        "]\n",
        "print(\"cell done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzKfaVaYWzHV",
        "outputId": "d6bc1dbe-3ad9-4cfb-e3b7-edff0dd0d5d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_w_ngrams = corpus.copy()\n",
        "corpus_w_ngrams = corpus_w_ngrams.map(lambda x: merge_ngram(x, bigrams, trigrams))\n",
        "\n",
        "print(\"cell done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2DNwVmZW1Uv",
        "outputId": "3b692c19-4e64-47bb-9b9e-e44080d353c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = multiprocessing.Pool()\n",
        "corpus_w_ngrams = p.map(filter_stopwords, [doc for doc in corpus_w_ngrams])\n",
        "p.close()\n",
        "print(\"cell done\") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMkCUcs_W3NM",
        "outputId": "8e9609f1-e290-4694-9fec-bd738057ef42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = multiprocessing.Pool()\n",
        "final_corpus = p.map(filter_pos, [doc for doc in corpus_w_ngrams])\n",
        "p.close()\n",
        "print(\"cell done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5t5HGLhXIao",
        "outputId": "ae21b800-ef70-4996-849d-e4b337d19445"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = gensim.corpora.Dictionary(final_corpus)\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.20)\n",
        "corpus_bow = [dictionary.doc2bow(doc) for doc in final_corpus]\n",
        "print(\"cell done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9ENqZd7XK0L",
        "outputId": "a32e9343-d8f3-4942-8962-e7ee2ffa090b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make output directory \n",
        "%mkdir ./lda_tsne"
      ],
      "metadata": {
        "id": "cJCc3JKIbml6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherence = []\n",
        "n_tops = [7, 10, 13, 15, 19] # these are topics chosen by jesse\n",
        "for ii in n_tops:\n",
        "    print(\"lda with {} topics\".format(ii))\n",
        "    Lda = gensim.models.ldamulticore.LdaMulticore\n",
        "    ldamodel = Lda(\n",
        "        corpus_bow,\n",
        "        num_topics=ii,\n",
        "        id2word=dictionary,\n",
        "        passes=40,\n",
        "        iterations=200,\n",
        "        chunksize=100,  # probably want to increase the chunksize?\n",
        "        eval_every=None,\n",
        "    )\n",
        "    print(\"fit model, computing coherence\")\n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(\n",
        "        model=ldamodel,\n",
        "        texts=final_corpus,\n",
        "        dictionary=dictionary,\n",
        "        coherence=\"c_v\",\n",
        "    )\n",
        "    coherence.append((ii,cm.get_coherence()))\n",
        "    print(\"generating tsne viz\")\n",
        "    p = pyLDAvis.gensim.prepare(\n",
        "        ldamodel, corpus_bow, dictionary, mds=\"tsne\"\n",
        "    )\n",
        "    title = \"./lda_tsne/topic_tsne_{}.html\".format(ii)\n",
        "    pyLDAvis.save_html(p, title)\n",
        "    dist = []\n",
        "    for doc in final_corpus:\n",
        "        bow = dictionary.doc2bow(doc)\n",
        "        dist.append(ldamodel.get_document_topics(bow, minimum_probability=0, minimum_phi_value=None, per_word_topics = False))\n",
        "    col = \"topic_dist_{}\".format(ii)\n",
        "    df[col] = dist\n",
        "    print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh6loGJYbXOZ",
        "outputId": "4d12cdd2-76d4-4813-d7ed-7698d77d8bbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lda with 7 topics\n",
            "fit model, computing coherence\n",
            "generating tsne viz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:699: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "lda with 10 topics\n",
            "fit model, computing coherence\n",
            "generating tsne viz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:699: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "lda with 13 topics\n",
            "fit model, computing coherence\n",
            "generating tsne viz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:699: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "lda with 15 topics\n",
            "fit model, computing coherence\n",
            "generating tsne viz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:699: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "lda with 19 topics\n",
            "fit model, computing coherence\n",
            "generating tsne viz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:699: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = [x[0] for x in coherence]\n",
        "cm = [x[1] for x in coherence]\n",
        "\n",
        "plt.plot(n_topics, cm)\n",
        "plt.scatter(n_topics, cm)\n",
        "plt.title(\"Number of Topics vs. Coherence\")\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence\")\n",
        "plt.xticks(n_topics)\n",
        "plt.savefig(\"./lda_tsne/topic_coherence_debug.png\")\n",
        "plt.close()\n",
        "\n",
        "df_exp = df.drop(columns='text')\n",
        "\n",
        "df_exp.to_csv(\"./lda_tsne/topic_dist.csv\")\n"
      ],
      "metadata": {
        "id": "2zyCftxFbewG"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}